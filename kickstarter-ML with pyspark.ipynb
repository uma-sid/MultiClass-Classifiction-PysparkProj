{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Pyspark","metadata":{}},{"cell_type":"markdown","source":"Install pyspark if its not already installed using the below code.","metadata":{}},{"cell_type":"code","source":"pip install pyspark","metadata":{"execution":{"iopub.status.busy":"2022-04-24T20:07:53.314583Z","iopub.execute_input":"2022-04-24T20:07:53.314919Z","iopub.status.idle":"2022-04-24T20:08:39.636382Z","shell.execute_reply.started":"2022-04-24T20:07:53.314831Z","shell.execute_reply":"2022-04-24T20:08:39.635020Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# 1. Importing the Libraries","metadata":{}},{"cell_type":"code","source":"from pyspark.sql import SparkSession\nfrom pyspark.conf import SparkConf\n\n# For scaling the data\nfrom pyspark.ml.feature import StandardScaler\n\n# For PCA\nfrom pyspark.ml.feature import PCA\n\n# for Clustering\nfrom pyspark.mllib.clustering import KMeans\nfrom pyspark.ml.clustering import KMeans\n\n# for dataframe operations\nfrom pyspark.sql.functions import when, lit, round, col, ceil\nfrom pyspark.sql.types import FloatType\n\n# for evaluating the clustering results\nfrom pyspark.ml.evaluation import ClusteringEvaluator\nfrom pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\nfrom numpy import array\nfrom math import sqrt\n\n# for balancing the data\nfrom pyspark.sql.functions import col, explode, array, lit\n\n# import Pipeline\nfrom pyspark.ml import Pipeline\n\n# import LogisticRegression classifier\nfrom pyspark.ml.classification import LogisticRegression\n\n# Import model evaluator\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\nfrom pyspark.mllib.evaluation import MulticlassMetrics\nfrom pyspark.mllib.evaluation import BinaryClassificationMetrics\nfrom pyspark.sql import SQLContext\n\n# importing the python related libraries for plotting\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n","metadata":{"execution":{"iopub.status.busy":"2022-04-24T20:08:39.641210Z","iopub.execute_input":"2022-04-24T20:08:39.641580Z","iopub.status.idle":"2022-04-24T20:08:40.955330Z","shell.execute_reply.started":"2022-04-24T20:08:39.641521Z","shell.execute_reply":"2022-04-24T20:08:40.954454Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"## creating the spark Context","metadata":{"execution":{"iopub.status.busy":"2022-04-14T19:55:35.034444Z","iopub.execute_input":"2022-04-14T19:55:35.034974Z","iopub.status.idle":"2022-04-14T19:55:35.040498Z","shell.execute_reply.started":"2022-04-14T19:55:35.034913Z","shell.execute_reply":"2022-04-14T19:55:35.039281Z"}}},{"cell_type":"code","source":"spark = SparkSession.builder\\\n.master(\"local[*]\")\\\n.appName(\"ML Implementation\")\\\n.getOrCreate()\nsc = spark.sparkContext","metadata":{"execution":{"iopub.status.busy":"2022-04-24T20:08:40.956873Z","iopub.execute_input":"2022-04-24T20:08:40.957339Z","iopub.status.idle":"2022-04-24T20:08:47.407961Z","shell.execute_reply.started":"2022-04-24T20:08:40.957301Z","shell.execute_reply":"2022-04-24T20:08:47.406817Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"We will first import the cleaned dataset exported from the EDA step of R environment.","metadata":{}},{"cell_type":"markdown","source":"# 2. Loading the data","metadata":{"execution":{"iopub.status.busy":"2022-04-14T19:55:58.222669Z","iopub.execute_input":"2022-04-14T19:55:58.222976Z","iopub.status.idle":"2022-04-14T19:55:58.227403Z","shell.execute_reply.started":"2022-04-14T19:55:58.222945Z","shell.execute_reply":"2022-04-14T19:55:58.226491Z"}}},{"cell_type":"code","source":"# Load and parse the data\nks_df = spark.read.csv(\"../input/kickstartercleandata/dataClean.csv\", header=True, inferSchema=True)\nks_df.printSchema()","metadata":{"execution":{"iopub.status.busy":"2022-04-24T20:08:47.412670Z","iopub.execute_input":"2022-04-24T20:08:47.413508Z","iopub.status.idle":"2022-04-24T20:08:54.920693Z","shell.execute_reply.started":"2022-04-24T20:08:47.413462Z","shell.execute_reply":"2022-04-24T20:08:54.919660Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"ks_df.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-24T20:08:54.921948Z","iopub.execute_input":"2022-04-24T20:08:54.922245Z","iopub.status.idle":"2022-04-24T20:08:55.507936Z","shell.execute_reply.started":"2022-04-24T20:08:54.922206Z","shell.execute_reply":"2022-04-24T20:08:55.506898Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"As we don't need some of the columns like launched, deadline and _c0 we will drop them along with the dates","metadata":{}},{"cell_type":"code","source":"# selecting only the necessary columns\nks_df = ks_df.select(['category', 'main_category','currency','backers','country','usd_pledged','usd_goal','launch_gap','state'])\nks_df.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-24T20:08:55.518049Z","iopub.execute_input":"2022-04-24T20:08:55.523375Z","iopub.status.idle":"2022-04-24T20:08:55.999268Z","shell.execute_reply.started":"2022-04-24T20:08:55.523300Z","shell.execute_reply":"2022-04-24T20:08:55.998265Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# making a list of columns \nall_cols = ks_df.columns\nall_cols","metadata":{"execution":{"iopub.status.busy":"2022-04-24T20:08:56.000470Z","iopub.execute_input":"2022-04-24T20:08:56.000774Z","iopub.status.idle":"2022-04-24T20:08:56.046207Z","shell.execute_reply.started":"2022-04-24T20:08:56.000736Z","shell.execute_reply":"2022-04-24T20:08:56.045253Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# looking at the modified schema\nks_df.printSchema()","metadata":{"execution":{"iopub.status.busy":"2022-04-24T20:08:56.047837Z","iopub.execute_input":"2022-04-24T20:08:56.048345Z","iopub.status.idle":"2022-04-24T20:08:56.055116Z","shell.execute_reply.started":"2022-04-24T20:08:56.048293Z","shell.execute_reply":"2022-04-24T20:08:56.054404Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# listing the categorical columns\ncat_cols = ['category','main_category','currency','country','state']\ncat_cols","metadata":{"execution":{"iopub.status.busy":"2022-04-24T20:08:56.056260Z","iopub.execute_input":"2022-04-24T20:08:56.056528Z","iopub.status.idle":"2022-04-24T20:08:56.071037Z","shell.execute_reply.started":"2022-04-24T20:08:56.056496Z","shell.execute_reply":"2022-04-24T20:08:56.069636Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# listing the category column names after indexing.\ncat_cols_indexed = ['category_indexed','main_category_indexed','currency_indexed','country_indexed', 'state_indexed']\ncat_cols_indexed","metadata":{"execution":{"iopub.status.busy":"2022-04-24T20:08:56.075445Z","iopub.execute_input":"2022-04-24T20:08:56.075977Z","iopub.status.idle":"2022-04-24T20:08:56.087973Z","shell.execute_reply.started":"2022-04-24T20:08:56.075924Z","shell.execute_reply":"2022-04-24T20:08:56.086923Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"## Creating a dataframe with numerical columns.","metadata":{"execution":{"iopub.status.busy":"2022-04-14T10:08:03.148136Z","iopub.execute_input":"2022-04-14T10:08:03.14883Z","iopub.status.idle":"2022-04-14T10:08:03.152891Z","shell.execute_reply.started":"2022-04-14T10:08:03.148787Z","shell.execute_reply":"2022-04-14T10:08:03.151961Z"}}},{"cell_type":"markdown","source":"We are going to perform PCA and do clustering on the PCA columns. As PCA is done on only on the continuous variables, we need to subset the numerical columns.","metadata":{"execution":{"iopub.status.busy":"2022-04-14T10:10:27.537056Z","iopub.execute_input":"2022-04-14T10:10:27.537351Z","iopub.status.idle":"2022-04-14T10:10:27.543992Z","shell.execute_reply.started":"2022-04-14T10:10:27.537319Z","shell.execute_reply":"2022-04-14T10:10:27.5426Z"}}},{"cell_type":"code","source":"# selecting only the necessary columns\nks_df_num = ks_df.select(['backers','usd_pledged','usd_goal','launch_gap'])\nks_df_num.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-24T20:08:56.091003Z","iopub.execute_input":"2022-04-24T20:08:56.091554Z","iopub.status.idle":"2022-04-24T20:08:56.374466Z","shell.execute_reply.started":"2022-04-24T20:08:56.091487Z","shell.execute_reply":"2022-04-24T20:08:56.373495Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"# 3. Scale the dataframe with numerical columns","metadata":{"execution":{"iopub.status.busy":"2022-04-14T10:10:46.163075Z","iopub.execute_input":"2022-04-14T10:10:46.163886Z","iopub.status.idle":"2022-04-14T10:10:46.167125Z","shell.execute_reply.started":"2022-04-14T10:10:46.163847Z","shell.execute_reply":"2022-04-14T10:10:46.166419Z"}}},{"cell_type":"code","source":"# Creating the dense vector of all the input features using vector assembler\nvector_assembler1 = VectorAssembler(inputCols=['backers','usd_pledged','usd_goal','launch_gap'], outputCol='features')","metadata":{"execution":{"iopub.status.busy":"2022-04-24T20:08:56.375865Z","iopub.execute_input":"2022-04-24T20:08:56.378192Z","iopub.status.idle":"2022-04-24T20:08:56.414958Z","shell.execute_reply.started":"2022-04-24T20:08:56.378130Z","shell.execute_reply":"2022-04-24T20:08:56.413767Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# transforming the data\nks_df_scaled1 = vector_assembler1.transform(ks_df)\nks_df_scaled1.show(2)","metadata":{"execution":{"iopub.status.busy":"2022-04-24T20:08:56.416293Z","iopub.execute_input":"2022-04-24T20:08:56.416642Z","iopub.status.idle":"2022-04-24T20:08:57.215818Z","shell.execute_reply.started":"2022-04-24T20:08:56.416599Z","shell.execute_reply":"2022-04-24T20:08:57.214899Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# Applying scaling on the features vector.\nstandard_scaler1 = StandardScaler(inputCol='features', outputCol='scaled_features')\nks_df_scaled1 = standard_scaler1.fit(ks_df_scaled1).transform(ks_df_scaled1)\nks_df_scaled1.show(5)","metadata":{"execution":{"iopub.status.busy":"2022-04-24T20:08:57.217054Z","iopub.execute_input":"2022-04-24T20:08:57.217412Z","iopub.status.idle":"2022-04-24T20:09:00.583356Z","shell.execute_reply.started":"2022-04-24T20:08:57.217366Z","shell.execute_reply":"2022-04-24T20:09:00.582528Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"## Performing PCA","metadata":{}},{"cell_type":"markdown","source":"We will first reduce the number of dimensions to two using the principal component analysis ","metadata":{}},{"cell_type":"code","source":"#Applying PCA\npca = PCA(k=2, inputCol='scaled_features', outputCol='pca')\nmodel = pca.fit(ks_df_scaled1)","metadata":{"execution":{"iopub.status.busy":"2022-04-24T20:09:00.584742Z","iopub.execute_input":"2022-04-24T20:09:00.585077Z","iopub.status.idle":"2022-04-24T20:09:04.912907Z","shell.execute_reply.started":"2022-04-24T20:09:00.585030Z","shell.execute_reply":"2022-04-24T20:09:04.912242Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# transform\nks_df_pca = model.transform(ks_df_scaled1)","metadata":{"execution":{"iopub.status.busy":"2022-04-24T20:09:04.913812Z","iopub.execute_input":"2022-04-24T20:09:04.914023Z","iopub.status.idle":"2022-04-24T20:09:05.010580Z","shell.execute_reply.started":"2022-04-24T20:09:04.913988Z","shell.execute_reply":"2022-04-24T20:09:05.009763Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# looking at the datatype of the object ks_df_pca\ntype(ks_df_pca)","metadata":{"execution":{"iopub.status.busy":"2022-04-24T20:09:05.011589Z","iopub.execute_input":"2022-04-24T20:09:05.011859Z","iopub.status.idle":"2022-04-24T20:09:05.025254Z","shell.execute_reply.started":"2022-04-24T20:09:05.011804Z","shell.execute_reply":"2022-04-24T20:09:05.023614Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# looking at the important columns\nks_df_pca.select(['features','scaled_features', 'pca', 'state']).show(5)","metadata":{"execution":{"iopub.status.busy":"2022-04-24T20:09:05.027086Z","iopub.execute_input":"2022-04-24T20:09:05.029744Z","iopub.status.idle":"2022-04-24T20:09:05.335579Z","shell.execute_reply.started":"2022-04-24T20:09:05.029686Z","shell.execute_reply":"2022-04-24T20:09:05.334606Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"# printing the pca vector\nks_df_pca.select(['pca']).show(truncate=False)","metadata":{"execution":{"iopub.status.busy":"2022-04-24T20:09:05.336826Z","iopub.execute_input":"2022-04-24T20:09:05.337142Z","iopub.status.idle":"2022-04-24T20:09:05.545712Z","shell.execute_reply.started":"2022-04-24T20:09:05.337100Z","shell.execute_reply":"2022-04-24T20:09:05.544707Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"Now, we have completed the PCA and reduced the dimension to two. We will now apply clustering using these principal componenets and try to visualize the clsters formed.","metadata":{"execution":{"iopub.status.busy":"2022-04-14T10:36:33.457105Z","iopub.execute_input":"2022-04-14T10:36:33.4574Z","iopub.status.idle":"2022-04-14T10:36:33.463557Z","shell.execute_reply.started":"2022-04-14T10:36:33.457369Z","shell.execute_reply":"2022-04-14T10:36:33.462714Z"}}},{"cell_type":"markdown","source":"# K-Means Clustering","metadata":{}},{"cell_type":"markdown","source":"We will use the K-Means Clustering algorithm to cluster the data","metadata":{}},{"cell_type":"markdown","source":"**Plan for Clustering**\n\n1. Find the ideal K-value, which represents the number of clusters.\n2. Clustering the data using the observed best k-value\n3. Evaluating the cluster results\n","metadata":{}},{"cell_type":"markdown","source":"**Finding out the best value for K(number of clusters)**","metadata":{}},{"cell_type":"code","source":"# Setting the parameters  and creating the object for the evaluator\neval = ClusteringEvaluator(predictionCol='prediction', featuresCol='pca', metricName='silhouette', distanceMeasure='squaredEuclidean')","metadata":{"execution":{"iopub.status.busy":"2022-04-24T20:09:05.546984Z","iopub.execute_input":"2022-04-24T20:09:05.547280Z","iopub.status.idle":"2022-04-24T20:09:05.563346Z","shell.execute_reply.started":"2022-04-24T20:09:05.547239Z","shell.execute_reply":"2022-04-24T20:09:05.562272Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"# Trying different possible k-values and finding out the silhouette score for each of those k-values.\nsilhouette_score = []\nprint(\"\"\"\nSilhoutte Scores for K Mean Clustering\n=======================================\nModel\\tScore\\t\n=====\\t=====\\t\n\"\"\")\nfor k in range(2,11):\n    kmeans_algo = KMeans(featuresCol='pca', k=k)\n    kmeans_fit = kmeans_algo.fit(ks_df_pca)\n    output = kmeans_fit.transform(ks_df_pca)\n    score = eval.evaluate(output)\n    silhouette_score.append(score)\n    #print(f\"K{k}\\t {round(score,2)}\\t\")\n    print(f\"K{k}\\t\", score,\"\\t\")","metadata":{"execution":{"iopub.status.busy":"2022-04-24T20:09:05.565099Z","iopub.execute_input":"2022-04-24T20:09:05.566194Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plotting the silhouette scores to identify the best k value\nimport matplotlib.pyplot as plt\nfig, ax = plt.subplots(1,1, figsize=(10,10))\nax.plot(range(2,11), silhouette_score)\nax.set_xlabel('K')\nax.set_ylabel('Score');","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the above silhoutte scores it is clear that at k=2 we have the best score. As this is a plot of Silhoutte score vs K we need to take the global maxima. So, we wil consider the best number of clusters as 2.","metadata":{}},{"cell_type":"markdown","source":"Now, lets cluster the data with k=2","metadata":{}},{"cell_type":"markdown","source":"**Train and Evaluate**","metadata":{}},{"cell_type":"code","source":"# Clustering the data using k=3\nkmeans = KMeans(featuresCol = 'pca', k=2)\nmodel = kmeans.fit(ks_df_pca)\nks_df_cls = model.transform(ks_df_pca)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluating the silihoutte score of the cluster\neval = ClusteringEvaluator(featuresCol='pca', metricName='silhouette', distanceMeasure='squaredEuclidean')\nsilhouette = eval.evaluate(ks_df_cls)\nprint(f\"Silhouette with squared euclidean distance: {silhouette}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Looking at the cluster centers\ncenters = model.clusterCenters()\nprint('Cluster Centers:')\nfor center in centers:\n    print(center)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ks_df_cls.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Column names for splitting the pca vector\ncolumn_names = ['pc1', 'pc2']\ncolumn_names","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Selecting only pca and state colummns for plotting.\nks_df_sub = ks_df_pca.select('pca','state')\nks_df_sub.show(5, truncate=False)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Splitting the vactor pca in to pc0 and pc1\nfrom pyspark.sql.functions import udf, col\nfrom pyspark.sql.types import ArrayType, DoubleType\n\ndef to_array(col):\n    def to_array_(v):\n        return v.toArray().tolist()\n    # Important: asNondeterministic requires Spark 2.3 or later\n    # It can be safely removed i.e.\n    # return udf(to_array_, ArrayType(DoubleType()))(col)\n    # but at the cost of decreased performance\n    return udf(to_array_, ArrayType(DoubleType())).asNondeterministic()(col)\n\nks_df_sub1 = (ks_df_sub\n    .withColumn(\"pc\", to_array(col(\"pca\")))\n    .select([\"state\"] + [col(\"pc\")[i] for i in range(2)]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ks_df_sub1.show(5, truncate=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plotting the formed clusters\nks_df_sub2 = ks_df_sub1.toPandas()\nsns.scatterplot(x='pc[0]', y='pc[1]', data=ks_df_sub2, hue='state')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As the clusters seems to be inseparable, let's look at the Silhouette Coefficient of the clusters and decide what's going on. ","metadata":{"execution":{"iopub.status.busy":"2022-04-14T11:11:56.361317Z","iopub.execute_input":"2022-04-14T11:11:56.361668Z","iopub.status.idle":"2022-04-14T11:11:56.368588Z","shell.execute_reply.started":"2022-04-14T11:11:56.361631Z","shell.execute_reply":"2022-04-14T11:11:56.367391Z"}}},{"cell_type":"code","source":"# Evaluating the silihoutte score of the cluster\neval = ClusteringEvaluator()\nsilhouette1 = eval.evaluate(ks_df_cls)\nprint(f\"Silhouette with squared euclidean distance: {silhouette1}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The Silhouette coefficient near to zero indicates that the clusters are inseparable or barely separable. It can be also be represented as the distance between the clusters is insignificant. But the score here is less than 0.4 so, we can conclude that the clusters are identifiable but the distance between them is not significant.","metadata":{"execution":{"iopub.status.busy":"2022-04-14T11:49:31.69398Z","iopub.execute_input":"2022-04-14T11:49:31.694289Z","iopub.status.idle":"2022-04-14T11:49:31.700382Z","shell.execute_reply.started":"2022-04-14T11:49:31.69425Z","shell.execute_reply":"2022-04-14T11:49:31.699433Z"}}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Machine Learning Implementation.","metadata":{}},{"cell_type":"markdown","source":"In the machine learning implementation we are going to use the multinomial Logistic Regression algorithm. As our data is having four different classes, we are going to use the technique of passing weights of the classes to the Logistic Regression algorithm to balance the dataset. \n\n**Plan for Machine Learning Implementation**\n1. One-hot encode the categorical columns\n2. Scale the data\n3. Calculate the weights of the classes\n4. Apply the multinomial Logistic Regression algorithm","metadata":{}},{"cell_type":"code","source":"# Looking into the dataframe we need to model\nks_df.show(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## One-Hot Encoding the categorical columns.","metadata":{}},{"cell_type":"markdown","source":"As there are many categorical columns, let's encode them using the one-hot encoder","metadata":{}},{"cell_type":"code","source":"# Creating the String indexer and fitting the data to it.\nindexer = StringIndexer(inputCols=cat_cols, outputCols=cat_cols_indexed)\nks_df = indexer.fit(ks_df).transform(ks_df)\nks_df.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# listing the category column names before encoding\ncat_cols_indexed_be = ['category_indexed','main_category_indexed','currency_indexed','country_indexed']\ncat_cols_indexed\n\n# listing the category column names after encoding.\ncat_cols_indexed_ae = ['category_indexed_O','main_category_indexed_O','currency_indexed_O','country_indexed_O']\ncat_cols_indexed_ae","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# One Hot encoding implementation using the indexed columns\nencoder = OneHotEncoder(inputCols=cat_cols_indexed_be, outputCols=cat_cols_indexed_ae)\nmodel =encoder.fit(ks_df)\nks_df = model.transform(ks_df)\nks_df.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now lets drop the cat_cols_indexed from our dataframe ks_df","metadata":{}},{"cell_type":"code","source":"# selecting only the necessary columns\nks_df = ks_df.select(['category', 'main_category','currency','backers','country','usd_pledged','usd_goal','launch_gap','state', 'category_indexed_O','main_category_indexed_O','currency_indexed_O','country_indexed_O'])\nks_df.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we have different ranges of values in the numerical columns we need to scale the data inorder to overcome the bias while machine learning model training.","metadata":{"execution":{"iopub.status.busy":"2022-04-11T22:44:49.680393Z","iopub.execute_input":"2022-04-11T22:44:49.680694Z","iopub.status.idle":"2022-04-11T22:44:49.68806Z","shell.execute_reply.started":"2022-04-11T22:44:49.680661Z","shell.execute_reply":"2022-04-11T22:44:49.686725Z"}}},{"cell_type":"markdown","source":"### Scaling the data","metadata":{}},{"cell_type":"code","source":"# making a list of columns \ninputcols = ['backers','usd_pledged','usd_goal','launch_gap','category_indexed_O','main_category_indexed_O','currency_indexed_O','country_indexed_O']\ninputcols","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating the dense vector of all the input features using vector assembler\nvector_assembler = VectorAssembler(inputCols=inputcols, outputCol='features')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# transforming the data\nks_df_scaled = vector_assembler.transform(ks_df)\nks_df_scaled.show(2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will use the standardscaler from the pyspark.ml.feature","metadata":{}},{"cell_type":"code","source":"# Applying scaling on the features vector.\nstandard_scaler = StandardScaler(inputCol='features', outputCol='scaled_features')\nks_df_scaled = standard_scaler.fit(ks_df_scaled).transform(ks_df_scaled)\nks_df_scaled.show(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Slicing the dataframe to have only the scaled_features and labels.\nks_df_ss = ks_df_scaled.selectExpr(\"scaled_features as features\", \"state as state\")\nks_df_ss.show(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Inspecting the Balance of the dataset","metadata":{}},{"cell_type":"markdown","source":"As we are dealing with a multi-class classification, we need to look at the balance of the dataset and try to balance it if it is imbalanced. ","metadata":{}},{"cell_type":"code","source":"ks_df_ss.groupBy('state').count().orderBy(col('count').desc()).show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we have an imbalanced dataset. Let's try to balance it using Oversampling technique","metadata":{"execution":{"iopub.status.busy":"2022-04-23T13:23:45.958527Z","iopub.execute_input":"2022-04-23T13:23:45.958794Z","iopub.status.idle":"2022-04-23T13:23:45.969181Z","shell.execute_reply.started":"2022-04-23T13:23:45.958752Z","shell.execute_reply":"2022-04-23T13:23:45.966249Z"}}},{"cell_type":"markdown","source":"# Balancing the data using Oversampling","metadata":{}},{"cell_type":"code","source":"# Calculating the ratio of weights to oversample\nfailed_df = ks_df_ss.filter(col(\"state\") == 'failed')\nsuccessful_df = ks_df_ss.filter(col(\"state\") == 'successful')\ncanceled_df = ks_df_ss.filter(col(\"state\") == 'canceled')\nsuspended_df = ks_df_ss.filter(col(\"state\") == 'suspended')\n\nratio_fai_suc = int(failed_df.count()/successful_df.count())\nratio_fai_can = int(failed_df.count()/canceled_df.count())\nratio_fai_sus = int(failed_df.count()/suspended_df.count())\n\nprint(\"ratio_fai_suc: {}\".format(ratio_fai_suc))\nprint(\"ratio_fai_can: {}\".format(ratio_fai_can))\nprint(\"ratio_fai_sus: {}\".format(ratio_fai_sus))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The actual 'failed' state records are almost 2 times higher than the 'successful' records. As we got the ratio of successful projects to the failed classes as 1, we need to inspect the actual float number before rounding off and try to round it off to a higher number. To balance the data better.","metadata":{}},{"cell_type":"code","source":"# Inspecting the actual ration in float\nratio_fai_suc1 = float(failed_df.count()/successful_df.count())\nprint(ratio_fai_suc1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As the ratio is 1.76 which can be rounded of to 2 to balance the data better, we will use ratio_fai_suc+1 to oversample the data.","metadata":{}},{"cell_type":"markdown","source":"## Oversampling the data","metadata":{"execution":{"iopub.status.busy":"2022-04-23T11:34:28.166984Z","iopub.execute_input":"2022-04-23T11:34:28.167903Z","iopub.status.idle":"2022-04-23T11:34:28.172847Z","shell.execute_reply.started":"2022-04-23T11:34:28.167848Z","shell.execute_reply":"2022-04-23T11:34:28.171677Z"}}},{"cell_type":"code","source":"# duplicate the minority rows in Successful state\nos_suc_df = successful_df.withColumn(\"dummy\", explode(array([lit(x) for x in range(int(ratio_fai_suc+1))]))).drop('dummy')\n# combine both oversampled successful rows and previous majority rows \nfailed_succ_df = failed_df.unionAll(os_suc_df)\n\n# duplicate the minority rows in Canceled state\nos_can_df = canceled_df.withColumn(\"dummy\", explode(array([lit(x) for x in range(ratio_fai_can)]))).drop('dummy')\n# combine both oversampled canceled rows and previous majority rows \nfailed_succ_can_df = failed_succ_df.unionAll(os_can_df)\n\n# duplicate the minority rows in Suspended state\nos_sus_df = suspended_df.withColumn(\"dummy\", explode(array([lit(x) for x in range(ratio_fai_sus)]))).drop('dummy')\n# combine both oversampled suspended rows and previous majority rows \nks_df_os = failed_succ_can_df.unionAll(os_sus_df)\n\n\nks_df_os.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# inspecting the balance of the data after oversampling.\nks_df_os.groupBy('state').count().orderBy(col('count').desc()).show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now the data looks quite balanced with an acceptable variation among the counts.","metadata":{}},{"cell_type":"markdown","source":"## Indexing the State column","metadata":{"execution":{"iopub.status.busy":"2022-04-23T11:35:38.975545Z","iopub.execute_input":"2022-04-23T11:35:38.97595Z","iopub.status.idle":"2022-04-23T11:35:38.981312Z","shell.execute_reply.started":"2022-04-23T11:35:38.975908Z","shell.execute_reply":"2022-04-23T11:35:38.980155Z"}}},{"cell_type":"code","source":"# Creating the String indexer and fitting state column to it\nindexer = StringIndexer(inputCol='state', outputCol='state_indexed')\nks_df_sliced = indexer.fit(ks_df_os).transform(ks_df_os)\nks_df_sliced.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ks_df_sliced.filter(ks_df_sliced.state_indexed==0).show(1)\nks_df_sliced.filter(ks_df_sliced.state_indexed==1).show(1)\nks_df_sliced.filter(ks_df_sliced.state_indexed==2).show(1)\nks_df_sliced.filter(ks_df_sliced.state_indexed==3).show(1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the above step it is clear that we have labels setup as below.\n\n0 --> successful\n\n1 --> failed\n\n2 --> suspended\n\n3 --> canceled","metadata":{"execution":{"iopub.status.busy":"2022-04-15T17:14:31.163565Z","iopub.execute_input":"2022-04-15T17:14:31.163942Z","iopub.status.idle":"2022-04-15T17:14:31.170331Z","shell.execute_reply.started":"2022-04-15T17:14:31.163897Z","shell.execute_reply":"2022-04-15T17:14:31.169222Z"}}},{"cell_type":"code","source":"# Slicing the dataframe to have only the scaled_features and labels.\nks_df_sliced = ks_df_sliced.selectExpr(\"features as features\", \"state_indexed as labels\")\nks_df_sliced.show(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Checking the datatypes\nks_df_sliced.printSchema()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The weight column is read as string. we need to change it to float","metadata":{}},{"cell_type":"markdown","source":"## Splitting the dataset into train and test sets","metadata":{}},{"cell_type":"markdown","source":"Now, we will split the dataset in to train and test sets randomly.","metadata":{}},{"cell_type":"code","source":"train, test = ks_df_sliced.randomSplit([0.75, 0.25], seed = 100)\nprint(\"Training Dataset Count: \" + str(train.count()))\nprint(\"Test Dataset Count: \" + str(test.count()))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.groupBy('labels').count().orderBy(col('count').desc()).show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Logistic Regression Classifier","metadata":{}},{"cell_type":"code","source":"# configuring and training the Logistic Regression classifier using the training data\nlr = LogisticRegression(featuresCol = 'features', labelCol = 'labels', maxIter=10)\n#lr.setWeightCol(\"weight\")\nlrModel = lr.fit(train)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Print the coefficients and intercept for multinomial logistic regression\nprint(\"Coefficients: \\n\" + str(lrModel.coefficientMatrix))\nprint(\"Intercept: \" + str(lrModel.interceptVector))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Performance Metrics","metadata":{}},{"cell_type":"code","source":"# Getting the training summary.\ntrainingSummary = lrModel.summary","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Obtain the objective per iteration\nobjectiveHistory = trainingSummary.objectiveHistory\nprint(\"objectiveHistory:\")\nfor objective in objectiveHistory:\n    print(objective)\n\n# for multiclass, we can inspect metrics on a per-label basis\nprint(\"False positive rate by label:\")\nfor i, rate in enumerate(trainingSummary.falsePositiveRateByLabel):\n    print(\"label %d: %s\" % (i, rate))\n\nprint(\"True positive rate by label:\")\nfor i, rate in enumerate(trainingSummary.truePositiveRateByLabel):\n    print(\"label %d: %s\" % (i, rate))\n\nprint(\"Precision by label:\")\nfor i, prec in enumerate(trainingSummary.precisionByLabel):\n    print(\"label %d: %s\" % (i, prec))\n\nprint(\"Recall by label:\")\nfor i, rec in enumerate(trainingSummary.recallByLabel):\n    print(\"label %d: %s\" % (i, rec))\n\nprint(\"F-measure by label:\")\nfor i, f in enumerate(trainingSummary.fMeasureByLabel()):\n    print(\"label %d: %s\" % (i, f))\n\naccuracy = trainingSummary.accuracy\nfalsePositiveRate = trainingSummary.weightedFalsePositiveRate\ntruePositiveRate = trainingSummary.weightedTruePositiveRate\nfMeasure = trainingSummary.weightedFMeasure()\nprecision = trainingSummary.weightedPrecision\nrecall = trainingSummary.weightedRecall\nprint(\"Accuracy: %s\\nFPR: %s\\nTPR: %s\\nF-measure: %s\\nPrecision: %s\\nRecall: %s\"\n          % (accuracy, falsePositiveRate, truePositiveRate, fMeasure, precision, recall))\n# $example off$","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Testing with the test dataset","metadata":{"execution":{"iopub.status.busy":"2022-04-15T17:44:14.908907Z","iopub.execute_input":"2022-04-15T17:44:14.909883Z","iopub.status.idle":"2022-04-15T17:44:14.918958Z","shell.execute_reply.started":"2022-04-15T17:44:14.90973Z","shell.execute_reply":"2022-04-15T17:44:14.918347Z"}}},{"cell_type":"code","source":"predictions = lrModel.transform(test)\npredictions.show(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Area under ROC of the model\nevaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\", labelCol='labels')\nprint('Test Area Under ROC', evaluator.evaluate(predictions))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions.show(20)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions.groupBy('labels', 'prediction').count().show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# compute TN, TP, FN, and FP\npredictions.groupBy('labels', 'prediction').count().show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds_and_labels = predictions.select(['prediction','labels']).withColumn('labels', col('labels').cast(FloatType())).orderBy('prediction')\n\n#select only prediction and label columns\npreds_and_labels = preds_and_labels.select(['prediction','labels'])\n\nmetrics = MulticlassMetrics(preds_and_labels.rdd.map(tuple))\n\nprint(metrics.confusionMatrix().toArray())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Class-0- Failed projects\nprint('\\n--------------Class-0 Successful Projects----------------')\nprint('True Positive Rate :', metrics.truePositiveRate(0.0))\nprint('False Positive Rate:', metrics.falsePositiveRate(0.0))\nprint('Precision          :', metrics.precision(0.0))\nprint('recall             :', metrics.recall(0.0))\nprint('f-measure          :', metrics.fMeasure(0.0))\n\n\n# Class-1- Successful projects\nprint('\\n--------------Class-1 Failed Projects----------------')\nprint('True Positive Rate :', metrics.truePositiveRate(1.0))\nprint('False Positive Rate:', metrics.falsePositiveRate(1.0))\nprint('Precision          :', metrics.precision(1.0))\nprint('recall             :', metrics.recall(1.0))\nprint('f-measure          :', metrics.fMeasure(1.0))\n\n# Class-2- Canclled projects\nprint('\\n--------------Class-2 Suspended Projects----------------')\nprint('True Positive Rate :', metrics.truePositiveRate(2.0))\nprint('False Positive Rate:', metrics.falsePositiveRate(2.0))\nprint('Precision          :', metrics.precision(2.0))\nprint('recall             :', metrics.recall(2.0))\nprint('f-measure          :', metrics.fMeasure(2.0))\n\n# Class-3- Suspended projects\nprint('\\n--------------Class-3 Canceled Projects----------------')\nprint('True Positive Rate :', metrics.truePositiveRate(3.0))\nprint('False Positive Rate:', metrics.falsePositiveRate(3.0))\nprint('Precision          :', metrics.precision(3.0))\nprint('recall             :', metrics.recall(3.0))\nprint('f-measure          :', metrics.fMeasure(3.0))\n\n# Overall Accuracy\nprint('\\n Overall Accuracy:',metrics.accuracy)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Teh model performed excellent in predicting the 'Successful' state projects. It did reasonable in predicting 'Failed' and 'Suspended' states but due to the low f-measure value, these states results can be interpreted as below par. But the model performed poor in predicting the 'canceled' state. ","metadata":{}},{"cell_type":"markdown","source":"As per the kickstarter's platform about section, the projects 'successful' and 'failure' states are important and the other two states are due to many other reasons outside the data. So, let's try and model the data as binary classification by only using 'successful' and 'failed' states.","metadata":{"execution":{"iopub.status.busy":"2022-04-23T11:43:41.698242Z","iopub.execute_input":"2022-04-23T11:43:41.69862Z","iopub.status.idle":"2022-04-23T11:43:41.705935Z","shell.execute_reply.started":"2022-04-23T11:43:41.69858Z","shell.execute_reply":"2022-04-23T11:43:41.704612Z"}}},{"cell_type":"markdown","source":"# Binary Classification Using only 'Successful' and 'Failed' state projects.","metadata":{}},{"cell_type":"code","source":"# Load and parse the data\nks_df1 = spark.read.csv(\"../input/kickstartercleandata/dataClean.csv\", header=True, inferSchema=True)\nks_df1.printSchema()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ks_df1.show(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we don't need some of the columns like launched, deadline and _c0 we will drop them along with the dates","metadata":{}},{"cell_type":"code","source":"# selecting only the necessary columns\nks_df1 = ks_df1.select(['category', 'main_category','currency','backers','country','usd_pledged','usd_goal','launch_gap','state'])\nks_df1.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# looking at the modified schema\nks_df1.printSchema()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dropping the 'Canceled' and 'Suspended' state records","metadata":{}},{"cell_type":"code","source":"ks_df1 = ks_df1.where((col('state')=='successful') | (col('state')=='failed'))\nks_df1.show(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ks_df1.groupBy('state').count().show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, we are only left with two states of records.","metadata":{}},{"cell_type":"code","source":"# making a list of columns \nall_cols = ks_df1.columns\nall_cols","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# listing the categorical columns\ncat_cols = ['category','main_category','currency','country','state']\ncat_cols","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# listing the category column names after indexing.\ncat_cols_indexed = ['category_indexed','main_category_indexed','currency_indexed','country_indexed', 'state_indexed']\ncat_cols_indexed","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Machine Learning Implementation for Binary Classification","metadata":{"execution":{"iopub.status.busy":"2022-04-15T22:38:34.035925Z","iopub.execute_input":"2022-04-15T22:38:34.036252Z","iopub.status.idle":"2022-04-15T22:38:34.040652Z","shell.execute_reply.started":"2022-04-15T22:38:34.036217Z","shell.execute_reply":"2022-04-15T22:38:34.039845Z"}}},{"cell_type":"markdown","source":"In the machine learning implementation we are going to use the multinomial Logistic Regression algorithm. As our data is having four different classes, we are going to use the technique of passing weights of the classes to the Logistic Regression algorithm to balance the dataset.\n\nPlan for Machine Learning Implementation\n\n1. One-hot encode the categorical columns\n2. Scale the data\n3. Calculate the weights of the classes\n4. Apply the multinomial Logistic Regression algorithm","metadata":{}},{"cell_type":"markdown","source":"## One-Hot Encoding the categorical columns.","metadata":{}},{"cell_type":"markdown","source":"As there are many categorical columns, let's encode them using the one-hot encoder","metadata":{}},{"cell_type":"code","source":"# Creating the String indexer and fitting the data to it.\nindexer = StringIndexer(inputCols=cat_cols, outputCols=cat_cols_indexed)\nks_df1 = indexer.fit(ks_df1).transform(ks_df1)\nks_df1.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# listing the category column names before encoding\ncat_cols_indexed_be = ['category_indexed','main_category_indexed','currency_indexed','country_indexed']\ncat_cols_indexed\n\n# listing the category column names after encoding.\ncat_cols_indexed_ae = ['category_indexed_O','main_category_indexed_O','currency_indexed_O','country_indexed_O']\ncat_cols_indexed_ae","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# One Hot encoding implementation using the indexed columns\nencoder = OneHotEncoder(inputCols=cat_cols_indexed_be, outputCols=cat_cols_indexed_ae)\nmodel =encoder.fit(ks_df1)\nks_df1 = model.transform(ks_df1)\nks_df1.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now lets drop the cat_cols_indexed from our dataframe ks_df","metadata":{"execution":{"iopub.status.busy":"2022-04-15T22:44:18.553372Z","iopub.execute_input":"2022-04-15T22:44:18.55368Z","iopub.status.idle":"2022-04-15T22:44:18.559614Z","shell.execute_reply.started":"2022-04-15T22:44:18.553645Z","shell.execute_reply":"2022-04-15T22:44:18.558468Z"}}},{"cell_type":"code","source":"# selecting only the necessary columns\nks_df1 = ks_df1.select(['category', 'main_category','currency','backers','country','usd_pledged','usd_goal','launch_gap','state', 'category_indexed_O','main_category_indexed_O','currency_indexed_O','country_indexed_O'])\nks_df1.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we have different ranges of values in the numerical columns we need to scale the data inorder to overcome the bias while machine learning model training.","metadata":{}},{"cell_type":"markdown","source":"## Scaling the data","metadata":{}},{"cell_type":"code","source":"# making a list of columns \ninputcols = ['backers','usd_pledged','usd_goal','launch_gap','category_indexed_O','main_category_indexed_O','currency_indexed_O','country_indexed_O']\ninputcols","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating the dense vector of all the input features using vector assembler\nvector_assembler = VectorAssembler(inputCols=inputcols, outputCol='features')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# transforming the data\nks_df_scaled = vector_assembler.transform(ks_df1)\nks_df_scaled.show(2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will use the standardscaler from the pyspark.ml.feature","metadata":{}},{"cell_type":"code","source":"# Applying scaling on the features vector.\nstandard_scaler = StandardScaler(inputCol='features', outputCol='scaled_features')\nks_df_scaled = standard_scaler.fit(ks_df_scaled).transform(ks_df_scaled)\nks_df_scaled.show(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Slicing the dataframe to have only the scaled_features and labels.\nks_df_ss = ks_df_scaled.selectExpr(\"scaled_features as features\", \"state as state\")\nks_df_ss.show(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Inspecting the Balance of the dataset","metadata":{}},{"cell_type":"markdown","source":"As we are dealing with a multi-class classification, we need to look at the balance of the dataset and try to balance it if it is imbalanced.","metadata":{}},{"cell_type":"code","source":"ks_df_ss.groupBy('state').count().orderBy(col('count').desc()).show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the above counts, it is evident that the data is imbalanced. So, we need to use a balancing technique while applying the machinelearning.\nSo, we will use the oversampling technique so that we won't loosing any data unlike the undersampling technique.","metadata":{}},{"cell_type":"markdown","source":"# Balancing the data using Oversampling","metadata":{}},{"cell_type":"code","source":"# Calculating the ratio of weights to oversample\nfailed_df = ks_df_ss.filter(col(\"state\") == 'failed')\nsuccessful_df = ks_df_ss.filter(col(\"state\") == 'successful')\n\nratio_fai_suc = int(failed_df.count()/successful_df.count())\n\nprint(\"ratio_fai_suc: {}\".format(ratio_fai_suc))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The actual 'failed' state records are almost 2 times higher than the 'successful' records. As we got the ratio of successful projects to the failed classes as 1, we need to inspect the actual float number before rounding off and try to round it off to a higher number. To balance the data better.","metadata":{}},{"cell_type":"code","source":"# Inspecting the actual ration in float\nratio_fai_suc1 = float(failed_df.count()/successful_df.count())\nprint(ratio_fai_suc1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As the ratio is 1.76 which can be rounded of to 2 to balance the data better, we will use ratio_fai_suc+1 to oversample the data.","metadata":{"execution":{"iopub.status.busy":"2022-04-23T11:50:50.266194Z","iopub.execute_input":"2022-04-23T11:50:50.266501Z","iopub.status.idle":"2022-04-23T11:50:50.273207Z","shell.execute_reply.started":"2022-04-23T11:50:50.266465Z","shell.execute_reply":"2022-04-23T11:50:50.271796Z"}}},{"cell_type":"markdown","source":"## Oversampling the data","metadata":{}},{"cell_type":"code","source":"# duplicate the minority rows in successful state\nos_suc_df = successful_df.withColumn(\"dummy\", explode(array([lit(x) for x in range(int(ratio_fai_suc+1))]))).drop('dummy')\n\n# combine both oversampled minority rows and previous majority rows \nks_df_os = failed_df.unionAll(os_suc_df)\n\nks_df_os.show(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# inspecting the balance of the data after oversampling.\nks_df_os.groupBy('state').count().orderBy(col('count').desc()).show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ks_df_os.show(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating the String indexer and fitting state column to it\nindexer = StringIndexer(inputCol='state', outputCol='state_indexed')\nks_df_sliced = indexer.fit(ks_df_os).transform(ks_df_os)\nks_df_sliced.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ks_df_sliced.filter(ks_df_sliced.state_indexed==0).show(1)\nks_df_sliced.filter(ks_df_sliced.state_indexed==1).show(1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the above step it is clear that we have labels setup as below.\n\n0 --> successful\n\n1 --> failed","metadata":{}},{"cell_type":"code","source":"# Slicing the dataframe to have only the scaled_features and labels.\nks_df_sliced = ks_df_sliced.selectExpr(\"features as features\", \"state_indexed as labels\")\nks_df_sliced.show(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Checking the datatypes\nks_df_sliced.printSchema()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Splitting the dataset into train and test sets","metadata":{}},{"cell_type":"markdown","source":"Now, we will split the dataset in to train and test sets randomly.","metadata":{}},{"cell_type":"code","source":"train, test = ks_df_sliced.randomSplit([0.75, 0.25], seed = 100)\nprint(\"Training Dataset Count: \" + str(train.count()))\nprint(\"Test Dataset Count: \" + str(test.count()))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Logistic Regression Classifier","metadata":{}},{"cell_type":"code","source":"# configuring and training the Logistic Regression classifier using the training data\nlr = LogisticRegression(featuresCol = 'features', labelCol = 'labels', maxIter=10)\n#lr.setWeightCol(\"weight\")\nlrModel = lr.fit(train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Print the coefficients and intercept for multinomial logistic regression\nprint(\"Coefficients: \\n\" + str(lrModel.coefficientMatrix))\nprint(\"Intercept: \" + str(lrModel.interceptVector))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Getting the training summary.\ntrainingSummary = lrModel.summary","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# calculate the statistics summary for the Logistic Regression model\ntrainingSummary = lrModel.summary\n# plot the ROC curve from the calculated summary\nroc = trainingSummary.roc.toPandas()\nplt.plot(roc['FPR'],roc['TPR'])\nplt.ylabel('False Positive Rate')\nplt.xlabel('True Positive Rate')\nplt.title('ROC Curve')\nplt.show()\nprint('Training set Area Under ROC: ' + str(trainingSummary.areaUnderROC))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pr = trainingSummary.pr.toPandas()\nplt.plot(pr['recall'],pr['precision'])\nplt.ylabel('Precision')\nplt.xlabel('Recall')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Obtain the objective per iteration\nobjectiveHistory = trainingSummary.objectiveHistory\nprint(\"objectiveHistory:\")\nfor objective in objectiveHistory:\n    print(objective)\n\n# for multiclass, we can inspect metrics on a per-label basis\nprint(\"False positive rate by label:\")\nfor i, rate in enumerate(trainingSummary.falsePositiveRateByLabel):\n    print(\"label %d: %s\" % (i, rate))\n\nprint(\"True positive rate by label:\")\nfor i, rate in enumerate(trainingSummary.truePositiveRateByLabel):\n    print(\"label %d: %s\" % (i, rate))\n\nprint(\"Precision by label:\")\nfor i, prec in enumerate(trainingSummary.precisionByLabel):\n    print(\"label %d: %s\" % (i, prec))\n\nprint(\"Recall by label:\")\nfor i, rec in enumerate(trainingSummary.recallByLabel):\n    print(\"label %d: %s\" % (i, rec))\n\nprint(\"F-measure by label:\")\nfor i, f in enumerate(trainingSummary.fMeasureByLabel()):\n    print(\"label %d: %s\" % (i, f))\n\naccuracy = trainingSummary.accuracy\nfalsePositiveRate = trainingSummary.weightedFalsePositiveRate\ntruePositiveRate = trainingSummary.weightedTruePositiveRate\nfMeasure = trainingSummary.weightedFMeasure()\nprecision = trainingSummary.weightedPrecision\nrecall = trainingSummary.weightedRecall\nprint(\"Accuracy: %s\\nFPR: %s\\nTPR: %s\\nF-measure: %s\\nPrecision: %s\\nRecall: %s\"\n          % (accuracy, falsePositiveRate, truePositiveRate, fMeasure, precision, recall))\n# $example off$","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Testing with the test dataset","metadata":{}},{"cell_type":"code","source":"predictions = lrModel.transform(test)\npredictions.show(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Area under ROC of the model\nevaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\", labelCol='labels')\nprint('Test Area Under ROC', evaluator.evaluate(predictions))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"evaluator = BinaryClassificationEvaluator(labelCol='labels')\nprint('Test Area Under ROC', evaluator.evaluate(predictions))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# computing TruePositive, TrueNegative, FalsePositive, FalseNegative\npredictions.groupBy('labels', 'prediction').count().show()\n# Calculate the elements of the confusion matrix\nTN = predictions.filter('prediction = 0 AND labels = prediction').count()\nTP = predictions.filter('prediction = 1 AND labels = prediction').count()\nFN = predictions.filter('prediction = 0 AND labels <> prediction').count()\nFP = predictions.filter('prediction = 1 AND labels <> prediction').count()\n# calculate accuracy, precision, recall, and F1-score\naccuracy = (TN + TP) / (TN + TP + FN + FP)\nprecision = TP / (TP + FP)\nrecall = TP / (TP + FN)\nF = 2 * (precision*recall) / (precision + recall)\nprint('precision:', precision)\nprint('recall   :', recall)\nprint('accuracy :', accuracy)\nprint('F1 score :', F)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So, with the above metrics, we can conclude that the data can be modeled with very good accuracy when we only use 'successful' and 'failed' state project records.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}